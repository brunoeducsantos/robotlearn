<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://brunoeducsantos.github.io/robotlearn/feed.xml" rel="self" type="application/atom+xml" /><link href="https://brunoeducsantos.github.io/robotlearn/" rel="alternate" type="text/html" /><updated>2021-01-09T08:40:15-06:00</updated><id>https://brunoeducsantos.github.io/robotlearn/feed.xml</id><title type="html">robotlearn</title><subtitle>A blog about robotics and online-learning</subtitle><entry><title type="html">Deep Learning applied to Follow-Me in robotics</title><link href="https://brunoeducsantos.github.io/robotlearn/deep%20learning/robotics/computer%20vision/online-learning/2020/01/09/deeplearningfollowrobotics.html" rel="alternate" type="text/html" title="Deep Learning applied to Follow-Me in robotics" /><published>2020-01-09T00:00:00-06:00</published><updated>2020-01-09T00:00:00-06:00</updated><id>https://brunoeducsantos.github.io/robotlearn/deep%20learning/robotics/computer%20vision/online-learning/2020/01/09/deeplearningfollowrobotics</id><content type="html" xml:base="https://brunoeducsantos.github.io/robotlearn/deep%20learning/robotics/computer%20vision/online-learning/2020/01/09/deeplearningfollowrobotics.html">&lt;h1 id=&quot;what-is-follow-me-in-robotics&quot;&gt;What is follow me in robotics?&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Follow Me&lt;/strong&gt; is a field in robotics to identify and track a target in the simulation.&lt;/p&gt;

&lt;p&gt;So-called “follow me” applications like this are key to many fields of robotics and the very same techniques you apply here could be extended to scenarios like advanced cruise control in autonomous vehicles or human-robot collaboration in the industry.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://github.com/brunoeducsantos/Follow-Me&quot;&gt;this project&lt;/a&gt;, a deep neural network was trained to identify and track a target in simulation, i.e., a person called “hero” which will be mixed with other people. In the figure below there is a representation of the drone following the “hero”.&lt;/p&gt;

&lt;h2 id=&quot;httpsbrunoeducsantosgithubiorobotlearnimagesfollowmepng&quot;&gt;“https://brunoeducsantos.github.io/robotlearn/images/followme.png”&lt;/h2&gt;

&lt;h2 id=&quot;have-you-heard-about-convolution-networks&quot;&gt;Have you heard about Convolution Networks?&lt;/h2&gt;

&lt;p&gt;Keeping in mind the concept of neural networks, there are two main advantages of using convolution networks applied to image recognition:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Parameter sharing: a feature detector (such as a vertical edge detector) that’s useful in one part of the image is probably useful in another part of the image.&lt;/li&gt;
  &lt;li&gt;Sparsity connections: in each layer, each output value depends only on a small number of inputs which makes it translation invariance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usually, convolution network architectures are structured as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs231n.github.io/convolutional-networks/#architectures&quot;&gt;Convolution Layer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs231n.github.io/convolutional-networks/#architectures&quot;&gt;Pooling Layer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs231n.github.io/#architectures&quot;&gt;Fully Connected Layer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/lecture/deep-neural-network/softmax-regression-HRy7y&quot;&gt;Softmax&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As activation function per a layer, RELU applies elementwise non-linearity.&lt;/p&gt;

&lt;p&gt;Even though convolution networks are the state of the art for object classification, for object detection, an architecture adjustment must be applied to provide pixelwise network learning. This architecture is so called Fully Convolution Network.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-is-a-fully-convolution-network-&quot;&gt;What is a Fully Convolution Network ?&lt;/h2&gt;

&lt;p&gt;A Fully Convolution Network (FCN) is a network architecture that allows preserving the spatial information throughout the network, which is very neat to object detection in an image. In addition, FCN can receive an input of any dimension.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fcn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Regarding the architecture, FCN has the following architecture :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Encoder (first 3 elements in the picture above)&lt;/li&gt;
  &lt;li&gt;1x1 convolution layer (4th item in the picture above)&lt;/li&gt;
  &lt;li&gt;Decoder ( 3 last times in the picture above)&lt;/li&gt;
  &lt;li&gt;Skip connection&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The encoder corresponds to the traditional convolution network described in the previous section. Encoder allows to learn the most important features to later on upsampling and obtain object segmentation in the image.&lt;/p&gt;

&lt;p&gt;A brand new concept is 1x1 convolution layer. In simple terms, it is a convolution layer that preserves spatial information . In addition 1x1 performs element wise multiplication and summation by applying a 1x1 sliding window over the input layers from the encoder. Finally,1x1 convolution reduces computational costs by reducing depth of encoder architecture. As an analogy, 1x1 convolution layer works like a Fully Connected Layer since combines linearly the depth layers and input a RELU , although reversely to Fully Connected Layers it preserves spatial information.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/decoder.png&quot; alt=&quot;decoder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, the decoder is composed by bilinear upsampling layers followed by convolution layer+ batch-normalization and skip connections with encoder layers to improve lost spatial features resolution.&lt;/p&gt;

&lt;p&gt;Let’s break down these three important concepts:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bilinear upsampling&lt;/li&gt;
  &lt;li&gt;Batch-normalization&lt;/li&gt;
  &lt;li&gt;Skip connections&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bilinear upsampling is a resampling technique that utilizes the weighted average of four nearest known pixels, located diagonally to a given pixel, to estimate a new pixel intensity value. The weighted average is usually distance dependent.&lt;/p&gt;

&lt;p&gt;Let’s consider the scenario where you have 4 known pixel values, so essentially a 2x2 grayscale image. This image is required to be upsampled to a 4x4 image. The following image gives a better idea of this process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/bilinear.png&quot; alt=&quot;bilinear&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s keep in mind though, the bilinear upsampling method does not contribute as a learnable layer like the transposed convolutions in the architecture and is prone to lose some finer details, but it helps speed up performance.&lt;/p&gt;

&lt;p&gt;For instance, transposed convolutions is a method that has learnable parameters instead of interpolation. A transposed convolution is somewhat similar because it produces the same spatial resolution a hypothetical deconvolutional layer would. However, the actual mathematical operation that’s being performed on the values is different. A transposed convolutional layer carries out a regular convolution but reverts its spatial transformation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/upsample.png&quot; alt=&quot;upsample&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For further detail about transposed convolution math go &lt;a href=&quot;https://arxiv.org/ftp/arxiv/papers/1609/1609.07009.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The second important concept is &lt;a href=&quot;https://www.coursera.org/lecture/deep-neural-network/why-does-batch-norm-work-81oTm&quot;&gt;batch-normalization&lt;/a&gt; which basically refers to normal data within the network per mini-batch of training data. This process allows the network to learn fast. In addition, it limit big changes in the activation functions inside the network, i.e., there is a more smooth and solid learning in the hidden layers.&lt;/p&gt;

&lt;p&gt;The third important concept is &lt;a href=&quot;https://www.coursera.org/lecture/convolutional-neural-networks/resnets-HAhz9&quot;&gt;skip-connections&lt;/a&gt;, which basically gets higher resolution from initial layers of encoder and combines with the decoder layers to get more spatial resolution lost during convolution layers . This method is important to the decoder since the upsampling doesn’t recover all the spatial information.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/skipconnection.png&quot; alt=&quot;Skip connection&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, &lt;a href=&quot;https://arxiv.org/pdf/1608.04117.pdf&quot;&gt;skip connections&lt;/a&gt; are also useful to train increasing deeper networks such as ResNet architecture.&lt;/p&gt;

&lt;p&gt;After building and training a FCN , a question arises naturally : how do we know if our object detection model is performing well? That is when Inserction Over Union(IoU) metric comes handy. In the next section we will discuss about this concept and why is a good metric for object detection performance.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;whats-about-iou&quot;&gt;What’s about IoU?&lt;/h2&gt;

&lt;p&gt;IoU measures how much the ground truth image overlaps with the segmented image resulting from our FCN model. Essentially, it measures the number of pixels inserction over pixel union from groundruth and segmented images FCN network. So, the metric mathematically formulation is :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;images/iou.png&quot; alt=&quot;iou&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now you are wondering, what about your model? What were the hurdles ? Which were the end results? Let’s talk about it in the next section.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-about-the-great-project&quot;&gt;What about the great project?&lt;/h2&gt;

&lt;p&gt;This project was divided into three main stages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Generate and pre-process the images from the simulator&lt;/li&gt;
  &lt;li&gt;Design the model architecture&lt;/li&gt;
  &lt;li&gt;Train and scoring the model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The preprocessing step transforms the depth masks from the sim, into binary masks suitable for training a neural network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/segm.png&quot; alt=&quot;segmentation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The final model architecture visualization is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/model.png&quot; alt=&quot;model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Firstly, the use of encoder and decoders to apply segmentation of objects in a image is based on pixel by pixel learning instead of image invariance filters as used in image classification where the spatial information is not so relevant. The overall strategy for deriving a model architecture began with a base on initial convolution layer of depth 32 with 3x3 filter , 1x1 convolution with depth 8 and decoder with same depth than encoder. The reason for this start was based on image input size 256X256X3. From this point, several convolution layers were added with increasing depth (based on powers of 2). This approach was based on SegNet architecture used by Stanford to segment objects in a image. It is important to mention that the 1x1 layer depth increase was correlated with data generation to reduce overfitting and model performance improvement. The data generation was important to reduce the error (cross-entropy) of training and validation datasets as well overcome the local minimum and allow the network to continue learning.&lt;/p&gt;

&lt;p&gt;Regarding the final score as you remember is defined by a metric called IoU, was given by 0.49. Usually, the criteria for validade a segmentation as the predicted object is &amp;gt;0.5. Although, for this project as a Udacity project, the criteria was lowered to 0.4.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/segm2.png&quot; alt=&quot;segmentation&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="deep learning" /><category term="robotics" /><category term="computer vision" /><category term="online-learning" /><summary type="html">What is follow me in robotics?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://brunoeducsantos.github.io/robotlearn/images/followme.png" /><media:content medium="image" url="https://brunoeducsantos.github.io/robotlearn/images/followme.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>